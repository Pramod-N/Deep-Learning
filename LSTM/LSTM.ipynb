{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f2c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f07360",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8580d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa0c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888ca3e",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f16992",
   "metadata": {},
   "source": [
    "#### one cell step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69062c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    xt = input at time step t (n_x,m)\n",
    "    aprev = activation output from previous step (n_a,m)\n",
    "    c_prev = cell state from prevous time step\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wu -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bu -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Retrieve the weights and biases from the parameters passed\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wu = parameters[\"Wu\"]\n",
    "    bu = parameters[\"bu\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    #retrieve the m,n_x and n_a\n",
    "    n_x,m = xt.shape\n",
    "    n_a,_ = a_prev.shape\n",
    "    \n",
    "    #concat aprev and xt\n",
    "    concated_data = np.zeros([n_x + n_a,m])\n",
    "    concated_data[:n_a,:] = a_prev\n",
    "    concated_data[n_a:,:] = xt\n",
    "    \n",
    "    cdasht = np.tanh( np.dot(Wc,concated_data) + bc)\n",
    "    \n",
    "    #forget gate\n",
    "    ft = sigmoid(np.dot(Wf,concated_data) + bf)\n",
    "    \n",
    "    #update gate\n",
    "    ut = sigmoid(np.dot(Wu,concated_data) + bu)\n",
    "    \n",
    "    #output gate\n",
    "    ot = sigmoid(np.dot(Wo,concated_data) + bo)\n",
    "    \n",
    "    #next memory cell output\n",
    "    c_next = ut*cdasht + ft*c_prev\n",
    "    \n",
    "    #activation output\n",
    "    a_next = ot*np.tanh(c_next)\n",
    "    \n",
    "    #ypred at time step t\n",
    "    yt_pred = softmax(np.dot(Wy,a_next) + by)\n",
    "    \n",
    "    #store data for backward propagation\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, ut, cdasht, ot, xt, parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54edf73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "c_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wu'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bu'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5671fab",
   "metadata": {},
   "source": [
    "#### forward steps(all cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7280d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    x = input of shape (n_x,m,T_x)\n",
    "    a0 = activation input of shape (n_a,m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wu -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bu -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #To keep track of list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    n_x,m,T_x = x.shape\n",
    "    n_a,_ = a0.shape\n",
    "    n_y,_ = parameters[\"Wy\"].shape\n",
    "    \n",
    "    #keep track of the activations, output and memory at each time step\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    y = np.zeros([n_y,m,T_x])\n",
    "    c = np.zeros([n_a,m,T_x])\n",
    "    \n",
    "    a_next = a0\n",
    "    c_next = np.zeros([n_a,m])\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        #one cell step\n",
    "        a_next,c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
    "        \n",
    "        #activation layer \n",
    "        a[:,:,t] = a_next\n",
    "        \n",
    "        #memory cell \n",
    "        c[:,:,t] = c_next\n",
    "        \n",
    "        #predictions\n",
    "        y[:,:,t] = yt\n",
    "        \n",
    "        #append the caches\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cc9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,7)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wu'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bu']= np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361f374",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd66c994",
   "metadata": {},
   "source": [
    "#### one cell step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05f86d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass \n",
    "    \"\"\"\n",
    "    \n",
    "    (a_next, c_next, a_prev, c_prev, ft, ut, cdasht, ot, xt, parameters) = cache\n",
    "    n_a,m = da_next.shape\n",
    "    #gate\n",
    "    dot = da_next * np.tanh(c_next)*ot*(1-ot)\n",
    "    dft = (dc_next*c_prev +(da_next * ot * (1 - np.tanh(c_next) ** 2)) * c_prev )* ft * (1 - ft)\n",
    "    dut = (dc_next*cdasht + (da_next * ot * (1 - np.tanh(c_next) ** 2)) * cdasht) * (1 - ut) * ut\n",
    "    dcdasht = (dc_next*ut + (da_next * ot * (1 - np.tanh(c_next) ** 2)) * ut) * (1 - cdasht ** 2)\n",
    "    \n",
    "    #bias\n",
    "    dbu = np.sum(dut,axis=1,keepdims=True)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcdasht,axis=1,keepdims=True)\n",
    "    \n",
    "    #weights\n",
    "    dWc = np.dot(dcdasht,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWu = np.dot(dut,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWf = np.dot(dft,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    dWo = np.dot(dot,np.concatenate((a_prev, xt), axis=0).T)\n",
    "    \n",
    "    da_prev = np.dot(parameters['Wf'][:,:n_a].T,dft)+np.dot(parameters['Wu'][:,:n_a].T,dut)+np.dot(parameters['Wc'][:,:n_a].T,dcdasht)+np.dot(parameters['Wo'][:,:n_a].T,dot) \n",
    "    \n",
    "    dc_prev = dc_next*ft+ot*(1-np.square(np.tanh(c_next)))*ft*da_next \n",
    "    \n",
    "    dxt = np.dot(parameters['Wf'][:,n_a:].T,dft)+np.dot(parameters['Wu'][:,n_a:].T,dut)+np.dot(parameters['Wc'][:,n_a:].T,dcdasht)+np.dot(parameters['Wo'][:,n_a:].T,dot) \n",
    "    \n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\"dbu\": dbu,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7467a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "c_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wu'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bu'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
    "\n",
    "da_next_tmp = np.random.randn(5,10)\n",
    "dc_next_tmp = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next_tmp, dc_next_tmp, cache_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e899113",
   "metadata": {},
   "source": [
    "#### Backward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b050cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \"\"\"\n",
    "    da = Gradients w.r.t hidden states of shape (n_a,m,T_x)\n",
    "    caches = output of lstm_forward\n",
    "    \"\"\"\n",
    "    #Retrieve the parameters\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cdash1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    #Get the shapes from the params\n",
    "    n_a,m,T_x = da.shape\n",
    "    n_x,m = x1.shape\n",
    "    \n",
    "    #Initialize the params\n",
    "    dWf = np.zeros((n_a, n_a + n_x))\n",
    "    dWu = np.zeros((n_a, n_a + n_x))\n",
    "    dWc = np.zeros((n_a, n_a + n_x))\n",
    "    dWo = np.zeros((n_a, n_a + n_x))\n",
    "    \n",
    "    dbf = np.zeros((n_a, 1))\n",
    "    dbu = np.zeros((n_a, 1))\n",
    "    dbc = np.zeros((n_a, 1))\n",
    "    dbo = np.zeros((n_a, 1))\n",
    "    \n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    dc_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    for t in reversed(range(T_x)):\n",
    "        #get the graidents\n",
    "        gradients = lstm_cell_backward(da[:,:,t]+da_prevt,dc_prevt, caches[t])\n",
    "        \n",
    "        dx[:,:,t] = gradients[\"dxt\"]\n",
    "        dWf += gradients[\"dWf\"]\n",
    "        dWu += gradients[\"dWu\"]\n",
    "        dWc += gradients[\"dWc\"]\n",
    "        dWo += gradients[\"dWo\"]\n",
    "        dbf += gradients[\"dbf\"]\n",
    "        dbu += gradients[\"dbu\"]\n",
    "        dbc += gradients[\"dbc\"]\n",
    "        dbo += gradients[\"dbo\"]\n",
    "        \n",
    "        da_prevt = gradients[\"da_prev\"]\n",
    "        dc_prevt = gradients[\"dc_prev\"]\n",
    "        \n",
    "    da0 = gradients[\"da_prev\"]\n",
    "    \n",
    "    # Store the gradients\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWu\": dWu,\"dbu\": dbu,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd275e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,7)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wu'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bu'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "\n",
    "da_tmp = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da_tmp, caches_tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
